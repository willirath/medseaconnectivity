{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "All trajectories are stored in a Google Cloud Storage bucket. We want to be able to load and filter all trajectories easily.  To this end, we load all the datasets (lazily), filter them to different parameters (starting MPA, depth, stokes drift), and store a Pandas dataframe with virtual sub-datasets for each combination of the parameters.  This Pandas dataframe will be pickled for later re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "dataset_version = \"v2019.09.11.2\"\n",
    "bucket_stokes = f\"pangeo-parcels/med_sea_connectivity_{dataset_version}/traj_data_with_stokes.zarr\"\n",
    "bucket_nostokes = f\"pangeo-parcels/med_sea_connectivity_{dataset_version}/traj_data_without_stokes.zarr\"\n",
    "\n",
    "filter_warnings = \"ignore\"  # No warnings will bother you.  Change for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all modules and spin up a Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from dask import array as da\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from gcsfs.mapping import GCSMap\n",
    "from xhistogram.xarray import histogram as xhist\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "\n",
    "from dask_kubernetes import KubeCluster\n",
    "cluster = KubeCluster(n_workers=8)\n",
    "cluster.adapt(minimum=8, maximum=60, wait_count=15)\n",
    "\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ☝️ Don't forget to click the link above to view the scheduler dashboard! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_dataset(bucket, restrict_to_MPA=None, restrict_to_z=None):\n",
    "    # load data\n",
    "    gcsmap = GCSMap(bucket)\n",
    "    ds = xr.open_zarr(gcsmap, decode_cf=False)\n",
    "    \n",
    "    # get info on starting region and make it an easy-to-look-up coord\n",
    "    initial_MPA = ds.MPA.isel(obs=0).squeeze()\n",
    "    ds.coords[\"initial_MPA\"] = initial_MPA\n",
    "     \n",
    "    # add mask that is False after land contact\n",
    "    ds[\"before_land_contact\"] = ((ds.land == 0).cumprod(\"obs\") == 1)\n",
    "      \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_stokes = open_dataset(bucket_stokes)\n",
    "ds_nostokes = open_dataset(bucket_nostokes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplify\n",
    "\n",
    "We know a few things about our data that make it easier to deal with them:\n",
    "\n",
    "- No vertical migration.  Hence, initial depth of a particle is valid for all times.\n",
    "\n",
    "- All time steps are the same. Hence, we can easily build a relative time axis that is valid for all particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_assumptions(ds):\n",
    "    \"\"\"Applies simplifications to the dataset that are valid for the \n",
    "    specific set of experiments we're dealing with here.\n",
    "    \n",
    "    Be careful when applying these to new experiments, because\n",
    "    they might not apply.\n",
    "    \"\"\"\n",
    "    # We assume no vertical migration and hence\n",
    "    # make (non-changing) depth level an easy to look up coord\n",
    "    z = ds.z.isel(obs=0).squeeze()\n",
    "    ds[\"z\"] = z\n",
    "    ds.coords[\"z\"] = ds.z\n",
    "    \n",
    "    # We assume that all time steps are equal\n",
    "    # and that the time axis is measured in seconds\n",
    "    # since some reference period\n",
    "    time_axis = ds.reset_coords([\"z\", \"initial_MPA\"]).time.isel(traj=0).squeeze()\n",
    "    time_axis -= time_axis.isel(obs=0).squeeze()\n",
    "    time_axis.attrs[\"units\"] = \"seconds since start of particle\"\n",
    "    ds.coords[\"time_axis\"] = time_axis\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_stokes = apply_assumptions(ds_stokes)\n",
    "ds_nostokes = apply_assumptions(ds_nostokes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load coordinates for quicker access\n",
    "\n",
    "So far, we did only the bare minimum of information (data types, variable names, number of time steps, ...) but did not load any of the data.  We want to continue to do so for the bulk of the data, but get coordinates and the like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_coords(ds, retries=40):\n",
    "    \"\"\"Will load coordinate data to the cluster.\"\"\"\n",
    "    ds[\"z\"] = ds[\"z\"].persist(retries=retries)\n",
    "    ds[\"initial_MPA\"] = ds[\"initial_MPA\"].persist(retries=retries)\n",
    "    ds[\"time_axis\"] = ds[\"time_axis\"].persist(retries=retries)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coords(ds, retries=40):\n",
    "    \"\"\"Will load coordinate data to the front end.\"\"\"\n",
    "    ds[\"z\"] = ds[\"z\"].compute(retries=retries)\n",
    "    ds[\"initial_MPA\"] = ds[\"initial_MPA\"].compute(retries=retries)\n",
    "    ds[\"time_axis\"] = ds[\"time_axis\"].compute(retries=retries)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_stokes = persist_coords(ds_stokes)\n",
    "ds_nostokes = persist_coords(ds_nostokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_stokes = compute_coords(ds_stokes)\n",
    "ds_nostokes = compute_coords(ds_nostokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_stokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_nostokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z_values(ds):\n",
    "    \"\"\"Load unique z-values to the front end.\n",
    "    \n",
    "    This triggers a computation across all of the z-level data.\n",
    "    \"\"\"\n",
    "    z_values = da.unique(ds.z.data).compute(retries=40)\n",
    "    z_values = z_values[~np.isnan(z_values)]\n",
    "    return z_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = get_z_values(ds_nostokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data\n",
    "\n",
    "We want to quickly select:\n",
    "- stokes drift on or off\n",
    "- MPA a trajectory started from\n",
    "- z-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_to(ds, MPA=None, z=None):\n",
    "    traj_indices = xr.full_like(ds.initial_MPA, True, dtype=\"bool\")\n",
    "    \n",
    "    if MPA is not None:\n",
    "        traj_indices = traj_indices & (ds.initial_MPA == MPA)\n",
    "    \n",
    "    if z is not None:\n",
    "        traj_indices = traj_indices & (ds.z == z)\n",
    "        \n",
    "    ds = ds.isel(traj=traj_indices)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_in_dataframe(ds, stokes=True, num_levels=1, num_mpas=9):\n",
    "    if num_levels is not None:\n",
    "        data = pd.DataFrame(\n",
    "            (\n",
    "                OrderedDict(\n",
    "                    {\n",
    "                        \"stokes\": stokes, \"MPA\": MPA, \"k\": k,\n",
    "                        \"data\": restrict_to(ds, MPA=MPA, z=z_values[k])\n",
    "                    }\n",
    "                )\n",
    "                for MPA in range(1, 1 + num_mpas)\n",
    "                for k in range(num_levels)\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        data = pd.DataFrame(\n",
    "            (\n",
    "                OrderedDict(\n",
    "                    {\n",
    "                        \"stokes\": stokes, \"MPA\": MPA, \"k\": -1,\n",
    "                        \"data\": restrict_to(ds, MPA=MPA, z=None)\n",
    "                    }\n",
    "                )\n",
    "                for MPA in range(1, 1 + num_mpas)\n",
    "            )\n",
    "        )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will trigger computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick-access dataframe for stokes drift data at surface\n",
    "data = wrap_in_dataframe(ds_stokes, stokes=True, num_levels=1, num_mpas=9)\n",
    "\n",
    "# add non-stokes data per level\n",
    "data = data.append(\n",
    "    wrap_in_dataframe(ds_nostokes, stokes=False, num_levels=len(z_values), num_mpas=9),\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# add non-stokes data without distinguishing levels\n",
    "data = data.append(\n",
    "    wrap_in_dataframe(ds_nostokes, stokes=False, num_levels=None, num_mpas=9),\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create thinned out data\n",
    "\n",
    "We're not sure if we need all the statistics.  Create sub-sampled datasets that only have 1%, 5%, and 10% of the data.  Sub-sampling is done randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thinned_data(ds, percent=50, seed=None):\n",
    "    \"\"\"Return dataset thinned to a percentage by randomly picking trajectories.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    traj_indices = (np.random.uniform(0, 1, size=ds.z.shape) < (percent / 100.0))\n",
    "    ds = ds.isel(traj=traj_indices)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for perc in [1, 5, 10]:\n",
    "    data[f\"thinned_data_{perc:03d}_percent\"] = data[\"data\"].apply(lambda ds: get_thinned_data(ds, percent=perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make it easy to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index(keys=[\"stokes\", \"MPA\", \"k\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_size(series):\n",
    "    return series.apply(lambda dobj: dobj.nbytes).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All data (doubly counting non-stokes data):\", get_total_size(data[\"data\"]) / 1e9, \"GB\")\n",
    "print(\"All thinned data (10%):\", get_total_size(data[\"thinned_data_010_percent\"]) / 1e9, \"GB\")\n",
    "print(\"All thinned data (5%):\", get_total_size(data[\"thinned_data_005_percent\"]) / 1e9, \"GB\")\n",
    "print(\"All thinned data (1%):\", get_total_size(data[\"thinned_data_001_percent\"]) / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(filter_warnings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store dataframe for later re-use.  Then re-load to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p intermediate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate_data/all_traj_dataframe.pickle\", mode=\"wb\") as f:\n",
    "    cloudpickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate_data/all_traj_dataframe.pickle\", mode=\"rb\") as f:\n",
    "    data = cloudpickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical documentation\n",
    "\n",
    "Lists the whole working environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda list --explicit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
